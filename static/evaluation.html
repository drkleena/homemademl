<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1" />

    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Palanquin" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Nunito+Sans" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Hind" rel="stylesheet">


    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    <link href="assets/css/prism.css" rel="stylesheet"/>

    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">

    <link href="assets/css/thebe_status_field.css" rel="stylesheet" type="text/css"/>
    <link rel="stylesheet" href = "./assets/css/post.css">

    <link rel = "https://cdnjs.cloudflare.com/ajax/libs/showdown/1.8.6/showdown.min.js">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <!-- THEBELAB STUFF -->
        <script type="text/javascript" src="https://unpkg.com/thebelab@^0.3.0"></script>

        <script src="assets/js/thebe_status_field.js" type="text/javascript" ></script>



        <script type="text/x-thebe-config">
          {
            binderOptions: {
              repo: "drkleena/HomeMadeML-Lesson-Material",
              ref: "master"
            },
            kernelOptions: {
              name: "python3",
            },
            requestKernel: true
          }
        </script>

    <script>
      window.onload = function () {
        thebe_place_activate_button();
        thebe_activate_button_function();
        thebe_activate_cells();
      };



      window.onscroll = function() {myFunction()};

      function myFunction() {
        var winScroll = document.body.scrollTop || document.documentElement.scrollTop;
        var height = document.documentElement.scrollHeight - document.documentElement.clientHeight;
        var scrolled = (winScroll / height) * 100;
        document.getElementById("myBar").style.width = scrolled + "%";
      }
    </script>







</head>

<body>
    <div class="header">
        <div class="progress-container">
            <div class="progress-bar" id="myBar"></div>
        </div>
    </div>

    <nav class="navbar navbar-expand-lg navbar-light pt-4 px-0 mx-5">
      <a class="navbar-brand" href="/">
        Homemade ML
      </a>

      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavDropdown" aria-controls="navbarNavDropdown" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNavDropdown">
        <ul class="navbar-nav">

        </ul>
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
              <div class="col-1" id = "status">
                  <p style = "font-weight: 5px; font-size: 1rem">Status: </p>
                  <span class="thebe_status_field"></span>
              </div>
          </li>

          <!-- <li class="nav-item">
            <a class="nav-link" href="https://www.facebook.com/designrevision"><i class="fa fa-facebook"></i></a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://dribbble.com/hisk"><i class="fa fa-dribbble"></i></a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="https://github.com/designrevision"><i class="fa fa-github"></i></a>
          </li>  -->
        </ul>
      </div>
    </nav>
    <div class=".container-fluid" style="width: 100%;">
        <div class="container">

        </div>

    </div>

    <div class=".container-fluid" id = "post_container" style="width: 100%;background-color:white;">

        <div class="container" id = heading>
            <h1>Model Evaluation</h1>

        </div>

        <div class="container" id = paragraph>
            <h3>Estimating our model's effectiveness</h3>
            <p>Often we hear alot of about neural networks,
            deep learning, convolutional networks
            and deep cross vectored reinforced bayes systems
            - well the last one was a joke but nevertheless it
            sounds pretty 'juicy'. Alot of the time people want
            to dive head first into these sexy machine learning
            techniques but neglect learning about the grunt work
            that comes with them. Evaluation is a crucial if not
            paramount step in machine learning, that along with
            preprocessing the data, takes an engineer the bulk of
            the effort trying to get right. Building a model is all
            well and good but if you cannot asses its performance in
            a meaningful and demonstrable way then it might as well be
            thrown in the trash. Throughout this series, we will take
            ourselfs through a journey of assesing a model under
            various fascets of evaluation and critically analyse these different methods.</p>
        </div>


        <div class="container" id = paragraph>

            <h3>Which instances to test? Which instances to train?</h3>

            <p>Lets say we have a model and mountains of data to train it on.
                One's first thought would be to simply train the model on the
                whole dataset and then test the model on the same dataset as a
                whole. Although this sounds great on paper it is deeply flawed
                and is the mortal sin in machine learning. If there's one thing
                to be taken away it is <b>do not test on training data</b> - ever.
                Imagine taking a subject where you learn the answers to a test
                prior to taking the test, would your knowledge be fairly assesed? -
                no definetely not. Therefore we must think of our task of
                training and evaluation as involving two seperate datasets
                (sampled from the same population - more on this later).
                This leads us to our first evaluation strategy - Holdout.</p>
        </div>

    <div class="container" id = paragraph>
            <h3>Holdout</h3>
            <p>What we do here is randomly sample x% of the dataset
            with no replacement and use this as train and the remaining instances as test.
            This leads to the testing dataset to be 'held out'. What we want to do is
            take a random selection of x% for training and 1-x% for testing.</p>
            <pre data-executable="true" data-language="python">%cd Chapter-1/Naive-Bayes
import pandas as pd
from collections import defaultdict as dd
import random
random.seed(3000)
import numpy as np
from NaiveBayes import *</pre>
<pre data-executable="true" data-language="python">
def preprocess(file, normal = True):
    if(normal):
        df = pd.read_csv("./datasets/"+file+".csv",header=None)
        unnamed = df.columns[len(df.columns)-1]
        df.rename(columns={unnamed:'class'},inplace=True)
    return df
</pre>




            <pre data-executable="true" data-language="python">def get_train_test_split(df, split):
    train_percent = split[0]
    test_percent = split[1]
    df_shuffled = df.sample(frac=1).reset_index(drop=True)
    train_portion_index = int(len(df_shuffled.index)*train_percent)
    test_portion_index = train_portion_index+1
    train_portion = df_shuffled[:train_portion_index].reset_index(drop=True)
    test_portion = df_shuffled[test_portion_index-1:].reset_index(drop=True)
    return train_portion, test_portion</pre>
<p>Run this snippet to see the accuracy of our previous Naive Bayes model. Try choosing different values for TRAIN_PERCENT.</p>
<pre data-executable="true" data-language="python">TRAIN = 0.8
TEST  = 1.0 - TRAIN

holdout_ratio = (TRAIN, TEST)

car_dataset = preprocess("car")
train, test = get_train_test_split(car_dataset,holdout_ratio)
nb_holdout = NaiveBayes(train)
results = nb_holdout.predict(test)

#lets see the accuracy
print ("SPLIT: {:f}/{:f} \nACCURACY: {}%".format(TRAIN*100,TEST*100,np.mean(results['predicted'] == results['class'])*100))
</pre>

            <p><b>We're done now right?</b>
            </br>
            no, simple holdout still gives us the
            burden of choosing the 'right' split ratio.
            A large amount of training data will allow the
            model to see more instances and thus become better
            trained. But with this comes a small amount of test
            data that doesn't really allow us to test the generalisability
            of our model, in other words, this split doesn't fairly evaluate our model.</p>
    </div>

    <div class="container" id = paragraph>
        <h3>Repeated Sub-sampling</h3>
        <p>Building on holdout, what we can do
        to get an evaluation metric with holdout
        with considerabily less evaluation variance
        is run holdout multiple times (keeping the split ratio constant)
        and averaging the accuracy metric for all runs
        </p>

        <pre data-executable="true" data-language="python">def repeated_sub_sampling(n, df, split):
    runs = []
    # returns the average of n holdout runs with the same split
    for run in range (n):
        train, test = get_train_test_split(df, split)
        nb_holdout = NaiveBayes(train)
        results = nb_holdout.predict(test)
        simple_accuracy = np.mean(results["class"]==results["predicted"])
        runs.append(simple_accuracy)
        print ("Run #{} {:f}".format(run+1,simple_accuracy*100))
    return np.mean(runs)</pre>




    <pre data-executable="true" data-language="python">RUNS = 5
print ("Final Accuracy: {:f}%".format(repeated_sub_sampling(RUNS, car_dataset, (TRAIN,TEST))*100))</pre>

</div>
    <div class="container" id = paragraph>
        <h3>Cross Validation</h3>
        <p>Cross validation allows us to test the skill of a model on unseen data.
        That is, it allows us to estimate the ability of a model to generalise to
        data not seen in the training set. Cross validation or CV for short,
        results in an evaluation that is less biased and optimistic than our
        first two methods of simply splitting train and test. The process is as follows:
        <ul>
            <li>Shuffle the dataset</li>
            <li>Split the dataset in to k unique groups</li>
            <li>Iteratively: for each k groups</li>
                <ul>
                    <li>Take a group as our test set, take the rest k-1 groups as training set</li>
                    <li>Train the model on train, and test on test</li>
                    <li>Get a model evaluation score (eg. simple accuracy) and discard the model</li>
                </ul>
            <li>Average the evaluation result of all k trials</li>
        </ul>
        </p>

    <pre data-executable="true" data-language="python">""" return folds takes the dataframe and merges the other k-1
	groups together, returns both the train group and the test group
"""
def return_folds(dfs, index):
    dfs_copy = list(dfs)
    test = dfs_copy.pop(index)
    train  = dfs_copy
    return (train,test)


def cross_validation(df, kfolds):

    df = df.sample(frac=1).reset_index(drop=True)

    accuracy_over_folds = []
    dfs = np.array_split(df, kfolds)



    for group_index in range(len(dfs)):

        #training
        train, test = return_folds(dfs, group_index)
        train = pd.concat(train, axis=0, join='outer', ignore_index=True)
        train = train.reset_index(drop=True)
        test = test.reset_index(drop=True)
        nb = NaiveBayes(train)
        predictions = nb.predict(test)

        # accuracy
        simple_accuracy = np.mean(predictions["class"]==predictions["predicted"])
        accuracy_over_folds.append(simple_accuracy)
        print("Computing CV- FOLD {}: {}%".format(group_index+1, 100*simple_accuracy))

    return np.mean(accuracy_over_folds)</pre>

<pre data-executable="true" data-language="python">print ("Final Accuracy: {:f}".format(cross_validation(car_dataset,10)*100))</pre>



<h4>Choosing k</h4>
<p><b>Large k </b></p>
<p>Large k values will be a more accurate representation of your model's skill but will take longer to compute.
As we increase k, we'll approach the most optimal scenario called 'leave one out' cross validation where
k equals the number of instances in our dataset. This means every instance gets used as a test set
exactly once which maximises our training set.</p>

<p><b>Small k </b></p>
<p>
Small k are quicker to compute but don't provide an accurate representation of the model's
skill and are prone to higher variance.This method of evaluation greatly decreases our
evaluation bias and variance. Evaluation bias being the propensity for our evaluation technique to
systematically over or under estimate our
model's performance, and evaluation variance being the spread in our estimations of accuracy over
independant trials.
</p>
</div>

    <footer>
      <nav class="navbar navbar-expand-lg">
          <!-- ^^ navbar-dark bg-dark  -->
        <div class="container">
          <a class="navbar-brand" href="/">Homemade ML</a>
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
          </button>
          <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav ml-auto">
              <!-- <li class="nav-item">
                <a class="nav-link" href="#">About</a>
              </li> -->
            </ul>
          </div>
        </div>
      </nav>
    </footer>

<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
<!-- <script src='https://cdnjs.cloudflare.com/ajax/libs/showdown/1.8.6/showdown.min.js'></script> -->
<!-- Causes a runtime error, hangs whole launching process -->
<!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script> -->

<script src="/assets/js/prism.js"></script>
<script type="text/javascript" src="https://unpkg.com/thebelab@^0.3.0"></script>
</body>
</html>
